{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "#from torchsampler import ImbalancedDatasetSampler\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print('cuda available: '+ str(torch.cuda.is_available()))\n",
    "#from skimage.morphology import disk, binary_dilation\n",
    "\n",
    "from models_avm import NestedUNet\n",
    "from loss_fun_avm import compute_per_channel_dice, DiceLoss, FocalLoss\n",
    "from tra_val_avm import train, validation\n",
    "from data_loader_avm import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tra1 = '/work/samhong833/Data_AVM/forUNetpp/1_tra/1'\n",
    "list_tra1 = os.listdir(path_tra1)\n",
    "for i in range(len(list_tra1)):\n",
    "    list_tra1[i] = path_tra1+'/'+list_tra1[i]\n",
    "    \n",
    "list_tra = list_tra1\n",
    "    \n",
    "path_val1 = '/work/samhong833/Data_AVM/forUNetpp/2_val/1'\n",
    "list_val1 = os.listdir(path_val1)\n",
    "for i in range(len(list_val1)):\n",
    "    list_val1[i] = path_val1+'/'+list_val1[i]\n",
    "    \n",
    "list_val = list_val1\n",
    "\n",
    "path_ts1 = '/work/samhong833/Data_AVM/forUNetpp/3_ts/1'\n",
    "list_ts1 = os.listdir(path_ts1)\n",
    "for i in range(len(list_ts1)):\n",
    "    list_ts1[i] = path_ts1+'/'+list_ts1[i]\n",
    "    \n",
    "list_ts = list_ts1\n",
    "\n",
    "path_tra_lab_txt = '/work/samhong833/Data_AVM/forYOLOv5/labels/1_tra'\n",
    "path_val_lab_txt = '/work/samhong833/Data_AVM/forYOLOv5/labels/2_val'\n",
    "path_ts_lab_txt = '/work/samhong833/Data_AVM/forYOLOv5/labels/3_ts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(list_tra,path_tra_lab_txt,rand_dilate=True,max_dilate_factor=50)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size = 12,\n",
    "    shuffle = True,   \n",
    ")\n",
    "\n",
    "val_data = Dataset(list_val,path_val_lab_txt,rand_dilate=True,max_dilate_factor=50)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset = val_data,           \n",
    "    batch_size = 4,                 \n",
    "    shuffle = False,              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model cuda for gpu\n",
    "model = NestedUNet(norm_method='batch').cuda()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.95)\n",
    "\n",
    "# loss function\n",
    "kwargs = {\"alpha\": 0.5, \"gamma\": 3, \"reduction\": 'mean'}\n",
    "criterion_FL = FocalLoss(**kwargs)\n",
    "criterion_DICE = DiceLoss()\n",
    "loss = [criterion_FL,criterion_DICE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  290 batches | lr 0.0001000 | ms/batch 138610.90 | loss_fl 0.007150 | loss_dice  0.29 | \n",
      "| epoch   1 |   200/  290 batches | lr 0.0001000 | ms/batch 137799.27 | loss_fl 0.004545 | loss_dice  0.27 | \n",
      "| epoch   1 |   290/  290 batches | lr 0.0001000 | ms/batch 123367.00 | loss_fl 0.003574 | loss_dice  0.26 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |   100/  118 batches | | ms/batch 24208.86 | val_loss_fl 0.001431 | val_loss_dice  0.21 | \n",
      "| epoch   1 |   118/  118 batches | | ms/batch 4209.08 | val_loss_fl 0.001433 | val_loss_dice  0.20 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 428.20s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  290 batches | lr 0.0001000 | ms/batch 112446.79 | loss_fl 0.001068 | loss_dice  0.20 | \n",
      "| epoch   2 |   200/  290 batches | lr 0.0001000 | ms/batch 116999.11 | loss_fl 0.001000 | loss_dice  0.19 | \n",
      "| epoch   2 |   290/  290 batches | lr 0.0001000 | ms/batch 113355.93 | loss_fl 0.000930 | loss_dice  0.19 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  118 batches | | ms/batch 19765.28 | val_loss_fl 0.000890 | val_loss_dice  0.16 | \n",
      "| epoch   2 |   118/  118 batches | | ms/batch 3419.69 | val_loss_fl 0.000884 | val_loss_dice  0.16 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 365.99s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  290 batches | lr 0.0001000 | ms/batch 109115.03 | loss_fl 0.000728 | loss_dice  0.17 | \n",
      "| epoch   3 |   200/  290 batches | lr 0.0001000 | ms/batch 110975.34 | loss_fl 0.000694 | loss_dice  0.17 | \n",
      "| epoch   3 |   290/  290 batches | lr 0.0001000 | ms/batch 105312.11 | loss_fl 0.000668 | loss_dice  0.17 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  118 batches | | ms/batch 16022.51 | val_loss_fl 0.000756 | val_loss_dice  0.16 | \n",
      "| epoch   3 |   118/  118 batches | | ms/batch 2801.60 | val_loss_fl 0.000756 | val_loss_dice  0.16 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 344.23s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  290 batches | lr 0.0001000 | ms/batch 109031.58 | loss_fl 0.000583 | loss_dice  0.15 | \n",
      "| epoch   4 |   200/  290 batches | lr 0.0001000 | ms/batch 109468.24 | loss_fl 0.000556 | loss_dice  0.15 | \n",
      "| epoch   4 |   290/  290 batches | lr 0.0001000 | ms/batch 102843.31 | loss_fl 0.000534 | loss_dice  0.15 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  118 batches | | ms/batch 16025.30 | val_loss_fl 0.000746 | val_loss_dice  0.17 | \n",
      "| epoch   4 |   118/  118 batches | | ms/batch 2820.35 | val_loss_fl 0.000737 | val_loss_dice  0.17 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 340.19s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  290 batches | lr 0.0001000 | ms/batch 109375.81 | loss_fl 0.000475 | loss_dice  0.14 | \n",
      "| epoch   5 |   200/  290 batches | lr 0.0001000 | ms/batch 110225.64 | loss_fl 0.000470 | loss_dice  0.14 | \n",
      "| epoch   5 |   290/  290 batches | lr 0.0001000 | ms/batch 101078.12 | loss_fl 0.000464 | loss_dice  0.14 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  118 batches | | ms/batch 15556.65 | val_loss_fl 0.000647 | val_loss_dice  0.16 | \n",
      "| epoch   5 |   118/  118 batches | | ms/batch 2825.50 | val_loss_fl 0.000650 | val_loss_dice  0.15 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 339.07s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  290 batches | lr 0.0001000 | ms/batch 109777.18 | loss_fl 0.000432 | loss_dice  0.14 | \n",
      "| epoch   6 |   200/  290 batches | lr 0.0001000 | ms/batch 110594.58 | loss_fl 0.000439 | loss_dice  0.14 | \n",
      "| epoch   6 |   290/  290 batches | lr 0.0001000 | ms/batch 102060.88 | loss_fl 0.000434 | loss_dice  0.14 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  118 batches | | ms/batch 15502.88 | val_loss_fl 0.001190 | val_loss_dice  0.23 | \n",
      "| epoch   6 |   118/  118 batches | | ms/batch 2830.70 | val_loss_fl 0.001169 | val_loss_dice  0.22 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 340.77s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  290 batches | lr 0.0001000 | ms/batch 109323.84 | loss_fl 0.000401 | loss_dice  0.13 | \n",
      "| epoch   7 |   200/  290 batches | lr 0.0001000 | ms/batch 111441.13 | loss_fl 0.000397 | loss_dice  0.13 | \n",
      "| epoch   7 |   290/  290 batches | lr 0.0001000 | ms/batch 103490.51 | loss_fl 0.000393 | loss_dice  0.13 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  118 batches | | ms/batch 15563.05 | val_loss_fl 0.001157 | val_loss_dice  0.23 | \n",
      "| epoch   7 |   118/  118 batches | | ms/batch 2786.69 | val_loss_fl 0.001136 | val_loss_dice  0.23 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 342.61s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  290 batches | lr 0.0001000 | ms/batch 109562.82 | loss_fl 0.000372 | loss_dice  0.12 | \n",
      "| epoch   8 |   200/  290 batches | lr 0.0001000 | ms/batch 111816.37 | loss_fl 0.000364 | loss_dice  0.12 | \n",
      "| epoch   8 |   290/  290 batches | lr 0.0001000 | ms/batch 104344.44 | loss_fl 0.000356 | loss_dice  0.12 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  118 batches | | ms/batch 15767.22 | val_loss_fl 0.000810 | val_loss_dice  0.18 | \n",
      "| epoch   8 |   118/  118 batches | | ms/batch 2837.46 | val_loss_fl 0.000808 | val_loss_dice  0.17 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 344.35s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  290 batches | lr 0.0001000 | ms/batch 109615.59 | loss_fl 0.000359 | loss_dice  0.12 | \n",
      "| epoch   9 |   200/  290 batches | lr 0.0001000 | ms/batch 111453.22 | loss_fl 0.000349 | loss_dice  0.12 | \n",
      "| epoch   9 |   290/  290 batches | lr 0.0001000 | ms/batch 104536.99 | loss_fl 0.000350 | loss_dice  0.12 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  118 batches | | ms/batch 15417.58 | val_loss_fl 0.000898 | val_loss_dice  0.19 | \n",
      "| epoch   9 |   118/  118 batches | | ms/batch 2772.86 | val_loss_fl 0.000877 | val_loss_dice  0.18 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 343.80s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  290 batches | lr 0.0001000 | ms/batch 109354.54 | loss_fl 0.000323 | loss_dice  0.12 | \n",
      "| epoch  10 |   200/  290 batches | lr 0.0001000 | ms/batch 111141.97 | loss_fl 0.000328 | loss_dice  0.12 | \n",
      "| epoch  10 |   290/  290 batches | lr 0.0001000 | ms/batch 103839.82 | loss_fl 0.000324 | loss_dice  0.12 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  118 batches | | ms/batch 15669.92 | val_loss_fl 0.001113 | val_loss_dice  0.17 | \n",
      "| epoch  10 |   118/  118 batches | | ms/batch 2796.83 | val_loss_fl 0.001105 | val_loss_dice  0.16 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 342.81s\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  11 |   100/  290 batches | lr 0.0000950 | ms/batch 109418.23 | loss_fl 0.000300 | loss_dice  0.11 | \n",
      "| epoch  11 |   200/  290 batches | lr 0.0000950 | ms/batch 111188.52 | loss_fl 0.000304 | loss_dice  0.11 | \n",
      "| epoch  11 |   290/  290 batches | lr 0.0000950 | ms/batch 103638.00 | loss_fl 0.000306 | loss_dice  0.11 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  118 batches | | ms/batch 16762.58 | val_loss_fl 0.000671 | val_loss_dice  0.15 | \n",
      "| epoch  11 |   118/  118 batches | | ms/batch 2943.32 | val_loss_fl 0.000666 | val_loss_dice  0.14 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 343.96s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  290 batches | lr 0.0000950 | ms/batch 109984.87 | loss_fl 0.000293 | loss_dice  0.11 | \n",
      "| epoch  12 |   200/  290 batches | lr 0.0000950 | ms/batch 110698.56 | loss_fl 0.000297 | loss_dice  0.11 | \n",
      "| epoch  12 |   290/  290 batches | lr 0.0000950 | ms/batch 103287.18 | loss_fl 0.000296 | loss_dice  0.11 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  118 batches | | ms/batch 15939.55 | val_loss_fl 0.001039 | val_loss_dice  0.16 | \n",
      "| epoch  12 |   118/  118 batches | | ms/batch 2766.07 | val_loss_fl 0.001022 | val_loss_dice  0.16 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 342.68s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  290 batches | lr 0.0000950 | ms/batch 109136.04 | loss_fl 0.000292 | loss_dice  0.11 | \n",
      "| epoch  13 |   200/  290 batches | lr 0.0000950 | ms/batch 111826.47 | loss_fl 0.000290 | loss_dice  0.11 | \n",
      "| epoch  13 |   290/  290 batches | lr 0.0000950 | ms/batch 104081.35 | loss_fl 0.000290 | loss_dice  0.11 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  118 batches | | ms/batch 15907.83 | val_loss_fl 0.000856 | val_loss_dice  0.15 | \n",
      "| epoch  13 |   118/  118 batches | | ms/batch 2755.15 | val_loss_fl 0.000843 | val_loss_dice  0.15 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 343.71s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  290 batches | lr 0.0000950 | ms/batch 109614.06 | loss_fl 0.000274 | loss_dice  0.10 | \n",
      "| epoch  14 |   200/  290 batches | lr 0.0000950 | ms/batch 110956.48 | loss_fl 0.000276 | loss_dice  0.10 | \n",
      "| epoch  14 |   290/  290 batches | lr 0.0000950 | ms/batch 104403.96 | loss_fl 0.000275 | loss_dice  0.10 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  118 batches | | ms/batch 16018.41 | val_loss_fl 0.000667 | val_loss_dice  0.15 | \n",
      "| epoch  14 |   118/  118 batches | | ms/batch 2919.14 | val_loss_fl 0.000658 | val_loss_dice  0.15 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 343.92s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  290 batches | lr 0.0000950 | ms/batch 109793.53 | loss_fl 0.000269 | loss_dice  0.10 | \n",
      "| epoch  15 |   200/  290 batches | lr 0.0000950 | ms/batch 110994.11 | loss_fl 0.000264 | loss_dice  0.10 | \n",
      "| epoch  15 |   290/  290 batches | lr 0.0000950 | ms/batch 103963.98 | loss_fl 0.000264 | loss_dice  0.10 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  118 batches | | ms/batch 16228.52 | val_loss_fl 0.001158 | val_loss_dice  0.17 | \n",
      "| epoch  15 |   118/  118 batches | | ms/batch 2996.13 | val_loss_fl 0.001143 | val_loss_dice  0.17 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 343.98s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  290 batches | lr 0.0000950 | ms/batch 109660.32 | loss_fl 0.000239 | loss_dice  0.09 | \n",
      "| epoch  16 |   200/  290 batches | lr 0.0000950 | ms/batch 110871.57 | loss_fl 0.000246 | loss_dice  0.09 | \n",
      "| epoch  16 |   290/  290 batches | lr 0.0000950 | ms/batch 103034.53 | loss_fl 0.000249 | loss_dice  0.10 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  118 batches | | ms/batch 16394.92 | val_loss_fl 0.000743 | val_loss_dice  0.14 | \n",
      "| epoch  16 |   118/  118 batches | | ms/batch 2856.19 | val_loss_fl 0.000732 | val_loss_dice  0.14 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 342.82s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/  290 batches | lr 0.0000950 | ms/batch 109053.15 | loss_fl 0.000248 | loss_dice  0.09 | \n"
     ]
    }
   ],
   "source": [
    "# Create Directory\n",
    "path = '/work/samhong833/Models/Seg_dia50_kneron'\n",
    "\n",
    "if os.path.isdir(path)==False:\n",
    "    os.mkdir(path)\n",
    "path = os.path.join(path,\"train\")\n",
    "if os.path.isdir(path)==False:\n",
    "    os.mkdir(path)       \n",
    "filenum = glob.glob(path + \"/exp*\")\n",
    "path = path + \"/exp\" + str(len(filenum)+1)\n",
    "os.mkdir(path)\n",
    "\n",
    "# Train the Model\n",
    "epochs = 500 # The number of epochs\n",
    "\n",
    "valloss = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(path=path,\n",
    "          model=model,\n",
    "          loss=loss,\n",
    "          optimizer=optimizer,\n",
    "          dataloader=train_loader,\n",
    "          epoch=epoch,\n",
    "          scheduler=scheduler)\n",
    "    print('-' * 89)\n",
    "    vallossnew = validation(path=path,\n",
    "          model=model,\n",
    "          loss=loss,\n",
    "          dataloader=val_loader,\n",
    "          epoch=epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    f2 = open(path + '/model_info.txt', 'a')\n",
    "    if vallossnew<valloss or epoch ==1: \n",
    "        fname = path + '/best_val'  + '.tar'\n",
    "        torch.save(model.state_dict(), fname)\n",
    "        valloss = vallossnew\n",
    "        f2.write('| best_val | epoch {:3d}| '.format(epoch)+'\\r\\n')        \n",
    "    f2.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log ^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
